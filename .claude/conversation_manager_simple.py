#!/usr/bin/env python3
"""
Claude Code å¯¹è¯å†å²è®°å½•ç®¡ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
ä¸éœ€è¦å¤–éƒ¨ä¾èµ–çš„ç‰ˆæœ¬
"""

import json
import os
import re
from typing import List, Dict, Any, Tuple
from datetime import datetime

class ConversationManagerSimple:
    """å¯¹è¯å†å²è®°å½•ç®¡ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, max_tokens: int = 100000, safety_margin: float = 0.1):
        """
        åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨

        Args:
            max_tokens: æœ€å¤§tokené™åˆ¶ï¼ˆClaude Sonnet 4.5çº¦128Kï¼‰
            safety_margin: å®‰å…¨è¾¹ç•Œæ¯”ä¾‹ï¼ˆåœ¨è¾¾åˆ°é™åˆ¶å‰å¼€å§‹æ¸…ç†ï¼‰
        """
        self.max_tokens = max_tokens
        self.safety_margin = safety_margin

        # é‡è¦å†…å®¹å…³é”®è¯ï¼ˆè¿™äº›å†…å®¹ä¼šè¢«ä¼˜å…ˆä¿ç•™ï¼‰
        self.important_keywords = [
            "ä»£ç ", "å®ç°", "ä¿®å¤", "bug", "é”™è¯¯", "åŠŸèƒ½", "éœ€æ±‚",
            "è®¾è®¡", "æ¶æ„", "æ–¹æ¡ˆ", "è®¡åˆ’", "todo", "ä»»åŠ¡",
            "æ–‡ä»¶", "è·¯å¾„", "ç›®å½•", "ç»“æ„", "é…ç½®",
            "æµ‹è¯•", "éªŒè¯", "æ£€æŸ¥", "é—®é¢˜", "è§£å†³",
            "ä¿®æ”¹", "æ›´æ–°", "æ·»åŠ ", "åˆ é™¤", "é‡æ„",
            "import", "def ", "class ", "function", "return",
            "TODO:", "FIXME:", "NOTE:", "WARNING:"
        ]

        # å¯æ¸…ç†å†…å®¹å…³é”®è¯ï¼ˆè¿™äº›å†…å®¹å¯ä»¥è¢«æ¸…ç†ï¼‰
        self.cleanable_keywords = [
            "ä½ å¥½", "è°¢è°¢", "è¯·é—®", "æ˜ç™½äº†", "ç†è§£äº†",
            "å¯¹çš„", "æ²¡é”™", "æ˜¯çš„", "ä¸æ˜¯", "å¯èƒ½",
            "æˆ‘è§‰å¾—", "æˆ‘è®¤ä¸º", "çœ‹èµ·æ¥", "ä¼¼ä¹",
            "é—²èŠ", "å¯¹è¯", "äº¤æµ", "è®¨è®º"
        ]

    def estimate_tokens(self, text: str) -> int:
        """ç®€åŒ–ç‰ˆtokenä¼°ç®—ï¼šä½¿ç”¨å­—ç¬¦æ•°/4ä½œä¸ºè¿‘ä¼¼å€¼"""
        # è‹±æ–‡æ–‡æœ¬å¤§çº¦1ä¸ªtokenå¯¹åº”4ä¸ªå­—ç¬¦
        # ä¸­æ–‡æ–‡æœ¬å¤§çº¦1ä¸ªtokenå¯¹åº”2-3ä¸ªå­—ç¬¦
        # è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°è®¡ï¼šå­—ç¬¦æ•°/3
        return len(text) // 3

    def analyze_conversation(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†æå¯¹è¯å†…å®¹ï¼Œè¯†åˆ«é‡è¦ä¿¡æ¯å’Œå¯æ¸…ç†å†…å®¹

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        analysis = {
            "total_tokens": 0,
            "important_messages": [],
            "cleanable_messages": [],
            "code_blocks": [],
            "file_references": [],
            "timeline": []
        }

        for i, msg in enumerate(messages):
            content = msg.get("content", "")
            role = msg.get("role", "")

            # ä¼°ç®—token
            tokens = self.estimate_tokens(content)
            analysis["total_tokens"] += tokens

            # åˆ†æå†…å®¹é‡è¦æ€§
            importance_score = self._calculate_importance(content, role)

            # è¯†åˆ«ä»£ç å—
            code_blocks = self._extract_code_blocks(content)
            if code_blocks:
                analysis["code_blocks"].extend(code_blocks)

            # è¯†åˆ«æ–‡ä»¶å¼•ç”¨
            file_refs = self._extract_file_references(content)
            if file_refs:
                analysis["file_references"].extend(file_refs)

            # è®°å½•æ—¶é—´çº¿
            analysis["timeline"].append({
                "index": i,
                "role": role,
                "tokens": tokens,
                "importance": importance_score,
                "has_code": len(code_blocks) > 0,
                "has_files": len(file_refs) > 0
            })

            # åˆ†ç±»æ¶ˆæ¯
            if importance_score >= 0.7:
                analysis["important_messages"].append(i)
            elif importance_score <= 0.3:
                analysis["cleanable_messages"].append(i)

        return analysis

    def _calculate_importance(self, content: str, role: str) -> float:
        """è®¡ç®—æ¶ˆæ¯çš„é‡è¦æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰"""
        score = 0.5  # åŸºç¡€åˆ†æ•°

        # è§’è‰²æƒé‡
        if role == "assistant":
            score += 0.1  # åŠ©æ‰‹çš„å›å¤é€šå¸¸æ›´é‡è¦
        elif role == "user":
            score += 0.05  # ç”¨æˆ·çš„æé—®

        # å†…å®¹ç‰¹å¾
        content_lower = content.lower()

        # æ£€æŸ¥é‡è¦å…³é”®è¯
        for keyword in self.important_keywords:
            if keyword.lower() in content_lower:
                score += 0.05

        # æ£€æŸ¥å¯æ¸…ç†å…³é”®è¯
        for keyword in self.cleanable_keywords:
            if keyword in content:
                score -= 0.03

        # ä»£ç å—æƒé‡
        if "```" in content:
            score += 0.2

        # æ–‡ä»¶è·¯å¾„æƒé‡
        if re.search(r'[/\\][\w\-\.]+[/\\]', content):
            score += 0.15

        # é™åˆ¶åœ¨0-1ä¹‹é—´
        return max(0.0, min(1.0, score))

    def _extract_code_blocks(self, content: str) -> List[Dict[str, Any]]:
        """ä»å†…å®¹ä¸­æå–ä»£ç å—"""
        code_blocks = []
        pattern = r'```(?:\w+)?\n(.*?)```'

        for match in re.finditer(pattern, content, re.DOTALL):
            code = match.group(1).strip()
            if code:
                code_blocks.append({
                    "content": code[:100] + "..." if len(code) > 100 else code,
                    "tokens": self.estimate_tokens(code)
                })

        return code_blocks

    def _extract_file_references(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–æ–‡ä»¶å¼•ç”¨"""
        # åŒ¹é…æ–‡ä»¶è·¯å¾„æ¨¡å¼
        patterns = [
            r'`([/\w\-\.]+\.\w+)`',  # åå¼•å·ä¸­çš„æ–‡ä»¶å
            r'\[([/\w\-\.]+\.\w+)\]',  # æ–¹æ‹¬å·ä¸­çš„æ–‡ä»¶å
            r'æ–‡ä»¶[ï¼š:]\s*([/\w\-\.]+\.\w+)',  # "æ–‡ä»¶: xxx"
            r'path[ï¼š:]\s*([/\w\-\./]+)',  # "path: xxx"
        ]

        file_refs = []
        for pattern in patterns:
            for match in re.finditer(pattern, content):
                file_ref = match.group(1)
                file_refs.append(file_ref)

        return file_refs

    def create_cleanup_plan(self, messages: List[Dict[str, Any]],
                           target_tokens: int = None) -> Dict[str, Any]:
        """
        åˆ›å»ºæ¸…ç†è®¡åˆ’

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            target_tokens: ç›®æ ‡tokenæ•°é‡ï¼ˆé»˜è®¤ä¿ç•™max_tokensçš„70%ï¼‰

        Returns:
            æ¸…ç†è®¡åˆ’å­—å…¸
        """
        if target_tokens is None:
            target_tokens = int(self.max_tokens * 0.7)

        analysis = self.analyze_conversation(messages)
        current_tokens = analysis["total_tokens"]

        if current_tokens <= target_tokens:
            return {
                "needs_cleanup": False,
                "current_tokens": current_tokens,
                "target_tokens": target_tokens,
                "messages_to_keep": list(range(len(messages))),
                "messages_to_remove": []
            }

        # éœ€è¦æ¸…ç†
        to_remove = []
        to_keep = list(range(len(messages)))

        # ç­–ç•¥1ï¼šä¼˜å…ˆæ¸…ç†ä½é‡è¦æ€§æ¶ˆæ¯
        for msg_idx in analysis["cleanable_messages"]:
            if current_tokens <= target_tokens:
                break

            timeline_info = next(t for t in analysis["timeline"] if t["index"] == msg_idx)
            current_tokens -= timeline_info["tokens"]
            to_remove.append(msg_idx)
            to_keep.remove(msg_idx)

        # ç­–ç•¥2ï¼šå¦‚æœè¿˜ä¸å¤Ÿï¼Œæ¸…ç†æ—©æœŸéé‡è¦æ¶ˆæ¯
        if current_tokens > target_tokens:
            # æŒ‰æ—¶é—´é¡ºåºï¼ˆä»æ—©åˆ°æ™šï¼‰æ¸…ç†éé‡è¦æ¶ˆæ¯
            for timeline_info in analysis["timeline"]:
                if current_tokens <= target_tokens:
                    break

                msg_idx = timeline_info["index"]
                if msg_idx in to_keep and timeline_info["importance"] < 0.6:
                    current_tokens -= timeline_info["tokens"]
                    to_remove.append(msg_idx)
                    to_keep.remove(msg_idx)

        # ç­–ç•¥3ï¼šä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼ˆæœ€å20%ï¼‰
        recent_cutoff = int(len(messages) * 0.8)
        recent_messages = [i for i in to_keep if i >= recent_cutoff]

        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€äº›æ¶ˆæ¯
        if len(to_keep) < 5:
            # ä¿ç•™æœ€é‡è¦çš„5æ¡æ¶ˆæ¯
            important_indices = sorted(analysis["important_messages"],
                                     key=lambda x: analysis["timeline"][x]["importance"],
                                     reverse=True)[:5]
            for idx in important_indices:
                if idx not in to_keep:
                    to_keep.append(idx)
                    if idx in to_remove:
                        to_remove.remove(idx)

        return {
            "needs_cleanup": True,
            "current_tokens": analysis["total_tokens"],
            "remaining_tokens": current_tokens,
            "target_tokens": target_tokens,
            "messages_to_keep": sorted(to_keep),
            "messages_to_remove": sorted(to_remove),
            "recent_messages": recent_messages,
            "important_messages": analysis["important_messages"]
        }

    def cleanup_conversation(self, messages: List[Dict[str, Any]],
                           plan: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ¸…ç†è®¡åˆ’

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            plan: æ¸…ç†è®¡åˆ’

        Returns:
            æ¸…ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not plan["needs_cleanup"]:
            return messages

        # åˆ›å»ºæ¸…ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        cleaned_messages = []
        summary_added = False

        for i, msg in enumerate(messages):
            if i in plan["messages_to_keep"]:
                cleaned_messages.append(msg)
            elif not summary_added:
                # åœ¨æ¸…ç†ç‚¹æ·»åŠ æ€»ç»“æ¶ˆæ¯
                summary_msg = self._create_summary_message(messages, plan)
                cleaned_messages.append(summary_msg)
                summary_added = True

        return cleaned_messages

    def _create_summary_message(self, messages: List[Dict[str, Any]],
                              plan: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæ¸…ç†æ€»ç»“æ¶ˆæ¯"""
        removed_count = len(plan["messages_to_remove"])
        kept_count = len(plan["messages_to_keep"])

        summary_content = f"""## å¯¹è¯å†å²è®°å½•æ¸…ç†æ€»ç»“

ä¸ºäº†ç¡®ä¿å¯¹è¯ä¸è¶…å‡ºtokené™åˆ¶ï¼Œå·²è‡ªåŠ¨æ¸…ç†å†å²è®°å½•ï¼š

### æ¸…ç†ç»Ÿè®¡
- åŸå§‹æ¶ˆæ¯æ•°é‡: {len(messages)}
- ä¿ç•™æ¶ˆæ¯æ•°é‡: {kept_count}
- æ¸…ç†æ¶ˆæ¯æ•°é‡: {removed_count}
- Tokenä½¿ç”¨: {plan['current_tokens']:,} â†’ {plan['remaining_tokens']:,}

### ä¿ç•™å†…å®¹
- é‡è¦ä»£ç ä¿®æ”¹å’Œå®ç°
- æ–‡ä»¶æ“ä½œå’Œè·¯å¾„å¼•ç”¨
- æœ€è¿‘çš„å¯¹è¯å†…å®¹
- å…³é”®å†³ç­–å’Œè®¡åˆ’

### æ¸…ç†å†…å®¹
- æ—©æœŸçš„é—²èŠå’Œéå…³é”®å¯¹è¯
- é‡å¤çš„è§£é‡Šå†…å®¹
- å·²å®Œæˆçš„ä¸´æ—¶è®¨è®º

### å½“å‰çŠ¶æ€
å¯¹è¯å·²ä¼˜åŒ–ï¼Œå¯ä»¥ç»§ç»­æ­£å¸¸å·¥ä½œã€‚å¦‚éœ€æŸ¥çœ‹å®Œæ•´å†å²ï¼Œè¯·å‘ŠçŸ¥ã€‚

---
*æ­¤æ¶ˆæ¯ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„æ¸…ç†æ€»ç»“*
"""

        return {
            "role": "system",
            "content": summary_content
        }

    def should_cleanup(self, messages: List[Dict[str, Any]]) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†"""
        analysis = self.analyze_conversation(messages)
        threshold = self.max_tokens * (1 - self.safety_margin)
        return analysis["total_tokens"] > threshold

    def get_conversation_stats(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
        analysis = self.analyze_conversation(messages)

        return {
            "total_messages": len(messages),
            "total_tokens": analysis["total_tokens"],
            "token_limit": self.max_tokens,
            "token_usage_percent": (analysis["total_tokens"] / self.max_tokens) * 100,
            "important_messages": len(analysis["important_messages"]),
            "code_blocks": len(analysis["code_blocks"]),
            "file_references": len(analysis["file_references"]),
            "needs_cleanup": self.should_cleanup(messages)
        }


def main():
    """æµ‹è¯•å‡½æ•°"""
    # ç¤ºä¾‹ç”¨æ³•
    manager = ConversationManagerSimple(max_tokens=100000)

    # æ¨¡æ‹Ÿä¸€äº›æ¶ˆæ¯
    test_messages = [
        {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æƒ³å¼€å‘ä¸€ä¸ªåŠŸèƒ½"},
        {"role": "assistant", "content": "å¥½çš„ï¼Œè¯·æè¿°ä½ çš„éœ€æ±‚ã€‚"},
        {"role": "user", "content": "æˆ‘éœ€è¦ä¸€ä¸ªæ–‡ä»¶ç®¡ç†å™¨ï¼Œå¯ä»¥å¤„ç† `/path/to/file.py`"},
        {"role": "assistant", "content": "```python\nimport os\n\ndef list_files(path):\n    return os.listdir(path)\n```"},
        {"role": "user", "content": "è°¢è°¢ï¼Œè¿™ä¸ªå¾ˆå¥½ç”¨"},
        {"role": "assistant", "content": "ä¸å®¢æ°”ï¼Œè¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿ"},
        {"role": "user", "content": "æˆ‘æƒ³æ·»åŠ ä¸€ä¸ªåŠŸèƒ½ï¼Œå¯ä»¥è¯»å– `/Users/grin/Documents/test.txt` æ–‡ä»¶"},
        {"role": "assistant", "content": "```python\ndef read_file(filepath):\n    with open(filepath, 'r') as f:\n        return f.read()\n```"},
        {"role": "user", "content": "è¿™ä¸ªå®ç°å¾ˆå¥½ï¼Œè°¢è°¢"},
        {"role": "assistant", "content": "è¿˜æœ‰å…¶ä»–éœ€è¦å¸®åŠ©çš„å—ï¼Ÿ"},
    ]

    print("=" * 60)
    print("å¯¹è¯å†å²è®°å½•ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 60)

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_conversation_stats(test_messages)
    print("\nğŸ“Š å¯¹è¯ç»Ÿè®¡:")
    print(f"   æ€»æ¶ˆæ¯æ•°: {stats['total_messages']}")
    print(f"   æ€»tokenæ•°: {stats['total_tokens']:,} / {stats['token_limit']:,}")
    print(f"   ä½¿ç”¨ç‡: {stats['token_usage_percent']:.1f}%")
    print(f"   é‡è¦æ¶ˆæ¯: {stats['important_messages']}")
    print(f"   ä»£ç å—: {stats['code_blocks']}")
    print(f"   æ–‡ä»¶å¼•ç”¨: {stats['file_references']}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
    if manager.should_cleanup(test_messages):
        print(f"\nâš ï¸  éœ€è¦æ¸…ç† (è¶…è¿‡å®‰å…¨é˜ˆå€¼)")

        # åˆ›å»ºæ¸…ç†è®¡åˆ’
        plan = manager.create_cleanup_plan(test_messages, target_tokens=500)

        print(f"\nğŸ“‹ æ¸…ç†è®¡åˆ’:")
        print(f"   åŸå§‹æ¶ˆæ¯: {len(test_messages)}")
        print(f"   ä¿ç•™æ¶ˆæ¯: {len(plan['messages_to_keep'])}")
        print(f"   æ¸…ç†æ¶ˆæ¯: {len(plan['messages_to_remove'])}")
        print(f"   Token: {plan['current_tokens']:,} â†’ {plan['remaining_tokens']:,}")

        # æ‰§è¡Œæ¸…ç†
        cleaned = manager.cleanup_conversation(test_messages, plan)
        print(f"\nâœ… æ¸…ç†å®Œæˆ:")
        print(f"   æ¸…ç†åæ¶ˆæ¯æ•°: {len(cleaned)}")

        # æ˜¾ç¤ºæ¸…ç†åçš„æ¶ˆæ¯æ‘˜è¦
        print(f"\nğŸ“ æ¸…ç†åæ¶ˆæ¯æ‘˜è¦:")
        for i, msg in enumerate(cleaned):
            role = msg.get("role", "unknown")
            content_preview = msg.get("content", "")[:80] + "..." if len(msg.get("content", "")) > 80 else msg.get("content", "")
            print(f"   [{i}] {role}: {content_preview}")
    else:
        print(f"\nâœ… çŠ¶æ€æ­£å¸¸ (æ— éœ€æ¸…ç†)")

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()