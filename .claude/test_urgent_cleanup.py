#!/usr/bin/env python3
"""
ç´§æ€¥æ¸…ç†æµ‹è¯•ï¼šæ¨¡æ‹Ÿå¯¹è¯å³å°†è¶…å‡ºtokené™åˆ¶çš„åœºæ™¯
"""

from conversation_manager_simple import ConversationManagerSimple

def create_critical_conversation():
    """åˆ›å»ºä¸€ä¸ªå³å°†è¶…å‡ºtokené™åˆ¶çš„ç´§æ€¥å¯¹è¯"""
    messages = []

    # æ·»åŠ å¤§é‡é•¿æ¶ˆæ¯ï¼Œæ¨¡æ‹Ÿé•¿æ—¶é—´å¼€å‘å¯¹è¯
    for i in range(100):
        # æ¯æ¡æ¶ˆæ¯éƒ½å¾ˆé•¿ï¼ŒåŒ…å«å¤§é‡å†…å®¹
        if i % 3 == 0:
            # è¶…é•¿ä»£ç æ¶ˆæ¯
            messages.append({
                "role": "assistant",
                "content": f"""```python
# æ¨¡å—{i//3+1} - å®Œæ•´å®ç°
{"#" * 80}
# è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ¨¡å—ï¼ŒåŒ…å«å¤šä¸ªç±»å’Œå‡½æ•°
# ç”¨äºå¤„ç†å¤æ‚çš„ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®è½¬æ¢
{"#" * 80}

import asyncio
import json
import logging
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

@dataclass
class DataRecord:
    \"\"\"æ•°æ®è®°å½•ç±»\"\"\"
    id: str
    timestamp: datetime
    value: float
    metadata: Dict[str, Any]
    tags: List[str]

    def to_dict(self) -> Dict[str, Any]:
        return {{
            'id': self.id,
            'timestamp': self.timestamp.isoformat(),
            'value': self.value,
            'metadata': self.metadata,
            'tags': self.tags
        }}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DataRecord':
        return cls(
            id=data['id'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            value=data['value'],
            metadata=data['metadata'],
            tags=data['tags']
        )

class DataProcessor:
    \"\"\"æ•°æ®å¤„ç†å™¨çš„æ ¸å¿ƒç±»\"\"\"

    def __init__(self, config_path: str = "/etc/app/config.json"):
        self.config_path = config_path
        self._load_config()
        self._cache = {{}}
        self._metrics = {{'processed': 0, 'errors': 0}}

    def _load_config(self):
        \"\"\"åŠ è½½é…ç½®æ–‡ä»¶\"\"\"
        try:
            with open(self.config_path, 'r') as f:
                self.config = json.load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {{self.config_path}}")
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {{e}}")
            self.config = {{'default': True}}

    async def process_batch(self, records: List[DataRecord]) -> List[Dict[str, Any]]:
        \"\"\"æ‰¹é‡å¤„ç†è®°å½•\"\"\"
        results = []
        tasks = [self._process_single(record) for record in records]

        for task in asyncio.as_completed(tasks):
            try:
                result = await task
                results.append(result)
                self._metrics['processed'] += 1
            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥: {{e}}")
                self._metrics['errors'] += 1

        return results

    async def _process_single(self, record: DataRecord) -> Dict[str, Any]:
        \"\"\"å¤„ç†å•ä¸ªè®°å½•\"\"\"
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ

        # å¤æ‚çš„ä¸šåŠ¡é€»è¾‘
        processed = {{
            'id': record.id,
            'processed_at': datetime.now().isoformat(),
            'original_value': record.value,
            'transformed_value': record.value * self.config.get('multiplier', 1.0),
            'has_metadata': bool(record.metadata),
            'tag_count': len(record.tags)
        }}

        # ç¼“å­˜ç»“æœ
        self._cache[record.id] = {{
            'record': record.to_dict(),
            'processed': processed,
            'timestamp': datetime.now()
        }}

        return processed

    def get_metrics(self) -> Dict[str, Any]:
        \"\"\"è·å–æ€§èƒ½æŒ‡æ ‡\"\"\"
        return {{
            **self._metrics,
            'cache_size': len(self._cache),
            'config_loaded': bool(self.config),
            'uptime': datetime.now() - self._start_time if hasattr(self, '_start_time') else timedelta(0)
        }}

    def cleanup_cache(self, max_age_hours: int = 24):
        \"\"\"æ¸…ç†ç¼“å­˜\"\"\"
        cutoff = datetime.now() - timedelta(hours=max_age_hours)
        to_remove = [
            key for key, value in self._cache.items()
            if value['timestamp'] < cutoff
        ]

        for key in to_remove:
            del self._cache[key]

        logger.info(f"æ¸…ç†äº† {{len(to_remove)}} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")

# å·¥å‚å‡½æ•°
def create_processor(config_path: Optional[str] = None) -> DataProcessor:
    \"\"\"åˆ›å»ºå¤„ç†å™¨å®ä¾‹\"\"\"
    return DataProcessor(config_path or "/etc/app/default_config.json")

# å•å…ƒæµ‹è¯•
import unittest

class TestDataProcessor(unittest.TestCase):
    \"\"\"å•å…ƒæµ‹è¯•ç±»\"\"\"

    def setUp(self):
        self.processor = create_processor()

    def test_process_single(self):
        \"\"\"æµ‹è¯•å•ä¸ªè®°å½•å¤„ç†\"\"\"
        record = DataRecord(
            id="test-1",
            timestamp=datetime.now(),
            value=100.0,
            metadata={{"source": "test"}},
            tags=["test", "unit"]
        )

        # æµ‹è¯•ä»£ç ...
        self.assertEqual(record.value, 100.0)

    def test_metrics(self):
        \"\"\"æµ‹è¯•æŒ‡æ ‡æ”¶é›†\"\"\"
        metrics = self.processor.get_metrics()
        self.assertIn('processed', metrics)
        self.assertIn('errors', metrics)

if __name__ == "__main__":
    # å¯åŠ¨å¤„ç†å™¨
    processor = create_processor()
    print(f"å¤„ç†å™¨å·²å¯åŠ¨ï¼Œé…ç½®è·¯å¾„: {{processor.config_path}}")

    # è¿è¡Œæµ‹è¯•
    unittest.main(argv=[''], exit=False)
```
è¿™æ˜¯ç¬¬{i//3+1}ä¸ªå®Œæ•´æ¨¡å—çš„å®ç°ï¼ŒåŒ…å«å¼‚æ­¥å¤„ç†ã€é…ç½®ç®¡ç†ã€ç¼“å­˜ç³»ç»Ÿå’Œå•å…ƒæµ‹è¯•ã€‚"""
            })
        else:
            # é•¿æ–‡æœ¬è®¨è®º
            messages.append({
                "role": "user",
                "content": f"""è¿™æ˜¯ç¬¬{i+1}æ¡è¯¦ç»†è®¨è®ºæ¶ˆæ¯ï¼Œæˆ‘ä»¬æ­£åœ¨æ·±å…¥æ¢è®¨ä¸€ä¸ªå¤æ‚çš„æŠ€æœ¯é—®é¢˜ã€‚

é—®é¢˜èƒŒæ™¯ï¼šæˆ‘ä»¬éœ€è¦è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œç”¨äºå¤„ç†å®æ—¶æ•°æ®æµã€‚ç³»ç»Ÿéœ€è¦æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

1. **å¯æ‰©å±•æ€§**ï¼šèƒ½å¤Ÿæ°´å¹³æ‰©å±•ä»¥å¤„ç†æ¯ç§’ç™¾ä¸‡çº§çš„äº‹ä»¶
2. **å®¹é”™æ€§**ï¼šå•ä¸ªèŠ‚ç‚¹æ•…éšœä¸å½±å“æ•´ä½“ç³»ç»Ÿè¿è¡Œ
3. **ä½å»¶è¿Ÿ**ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿå°äº100æ¯«ç§’
4. **æ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿æ•°æ®åœ¨ä¸åŒèŠ‚ç‚¹é—´çš„ä¸€è‡´æ€§
5. **ç›‘æ§å’Œå‘Šè­¦**ï¼šå®Œå–„çš„ç›‘æ§ä½“ç³»å’Œå®æ—¶å‘Šè­¦

æŠ€æœ¯é€‰å‹è€ƒè™‘ï¼š
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šKafka vs RabbitMQ vs Redis Streams
- æ•°æ®åº“ï¼šPostgreSQL vs Cassandra vs MongoDB
- ç¼“å­˜ï¼šRedis vs Memcached
- å®¹å™¨ç¼–æ’ï¼šKubernetes vs Docker Swarm

æ¶æ„è®¾è®¡è¦ç‚¹ï¼š
1. é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ¯ä¸ªæœåŠ¡ç‹¬ç«‹éƒ¨ç½²
2. ä½¿ç”¨APIç½‘å…³è¿›è¡Œè¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
3. å®ç°æœåŠ¡å‘ç°å’Œé…ç½®ä¸­å¿ƒ
4. å»ºç«‹å®Œå–„çš„æ—¥å¿—å’Œç›‘æ§ä½“ç³»
5. è®¾è®¡è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œå›æ»šæœºåˆ¶

å…·ä½“å®ç°æ­¥éª¤ï¼š
1. æ­å»ºåŸºç¡€æ¶æ„ï¼ˆKubernetesé›†ç¾¤ã€ç›‘æ§ç³»ç»Ÿï¼‰
2. å®ç°æ ¸å¿ƒæœåŠ¡ï¼ˆæ•°æ®é‡‡é›†ã€å¤„ç†ã€å­˜å‚¨ï¼‰
3. å¼€å‘ç®¡ç†ç•Œé¢å’ŒAPI
4. è¿›è¡Œæ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
5. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

è¿™æ˜¯ä¸€ä¸ªé•¿æœŸé¡¹ç›®ï¼Œé¢„è®¡éœ€è¦3-6ä¸ªæœˆå®Œæˆã€‚æˆ‘ä»¬éœ€è¦å®šæœŸreviewè¿›åº¦ï¼Œè°ƒæ•´æŠ€æœ¯æ–¹æ¡ˆã€‚

å½“å‰è¿›å±•ï¼šå·²å®ŒæˆæŠ€æœ¯é€‰å‹å’Œæ¶æ„è®¾è®¡ï¼Œå¼€å§‹æ­å»ºå¼€å‘ç¯å¢ƒã€‚

ä¸‹ä¸€æ­¥è®¡åˆ’ï¼š
1. æ­å»ºå¼€å‘ç¯å¢ƒï¼ˆæœ¬å‘¨ï¼‰
2. å®ç°æ ¸å¿ƒæ•°æ®æ¨¡å‹ï¼ˆä¸‹å‘¨ï¼‰
3. å¼€å‘ç¬¬ä¸€ä¸ªå¾®æœåŠ¡ï¼ˆä¸‹ä¸‹å‘¨ï¼‰

è¯·æä¾›å…·ä½“çš„æŠ€æœ¯å»ºè®®å’Œå®ç°ç»†èŠ‚ã€‚"""
            })

    return messages

def test_critical_scenario():
    """æµ‹è¯•ç´§æ€¥æ¸…ç†åœºæ™¯"""
    print("=" * 90)
    print("âš ï¸ ç´§æ€¥æ¸…ç†æµ‹è¯•ï¼šæ¨¡æ‹Ÿå¯¹è¯å³å°†è¶…å‡ºtokené™åˆ¶")
    print("=" * 90)

    # è®¾ç½®è¾ƒå°çš„tokené™åˆ¶ä»¥æ¨¡æ‹Ÿç´§æ€¥æƒ…å†µ
    MAX_TOKENS = 50000  # è¾ƒå°çš„é™åˆ¶
    manager = ConversationManagerSimple(max_tokens=MAX_TOKENS, safety_margin=0.1)  # è¾ƒå°çš„å®‰å…¨è¾¹ç•Œ

    print(f"\nğŸ¯ æµ‹è¯•é…ç½®:")
    print(f"   Tokené™åˆ¶: {MAX_TOKENS:,}")
    print(f"   å®‰å…¨è¾¹ç•Œ: {manager.safety_margin*100:.0f}%")
    print(f"   æ¸…ç†é˜ˆå€¼: {(1-manager.safety_margin)*100:.0f}%")

    # åˆ›å»ºç´§æ€¥å¯¹è¯
    print(f"\nğŸ“¥ åˆ›å»ºç´§æ€¥å¯¹è¯åœºæ™¯...")
    messages = create_critical_conversation()

    # æ‰‹åŠ¨è®¾ç½®é«˜tokenä½¿ç”¨ï¼ˆæ¨¡æ‹Ÿå®é™…æƒ…å†µï¼‰
    # ç”±äºæˆ‘ä»¬çš„ä¼°ç®—æ–¹æ³•ç®€å•ï¼Œè¿™é‡Œç›´æ¥è®¾ç½®ä¸€ä¸ªé«˜å€¼
    print(f"   åˆ›å»ºäº† {len(messages)} æ¡è¶…é•¿æ¶ˆæ¯")

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_conversation_stats(messages)

    print(f"\nğŸ“Š ç´§æ€¥çŠ¶æ€ç»Ÿè®¡:")
    print(f"   ğŸ“‹ æ€»æ¶ˆæ¯æ•°: {stats['total_messages']}")
    print(f"   ğŸ§® æ€»tokenæ•°: {stats['total_tokens']:,} / {stats['token_limit']:,}")
    print(f"   ğŸ“ˆ ä½¿ç”¨ç‡: {stats['token_usage_percent']:.1f}%")
    print(f"   â­ é‡è¦æ¶ˆæ¯: {stats['important_messages']}æ¡")
    print(f"   ğŸ’» ä»£ç å—: {stats['code_blocks']}ä¸ª")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç´§æ€¥æ¸…ç†
    needs_cleanup = stats['needs_cleanup']
    CLEANUP_THRESHOLD = (1 - manager.safety_margin) * 100

    if needs_cleanup:
        print(f"\nğŸš¨ ç´§æ€¥çŠ¶æ€: éœ€è¦ç«‹å³æ¸…ç†!")
        print(f"   åŸå› : ä½¿ç”¨ç‡ {stats['token_usage_percent']:.1f}% > ç´§æ€¥é˜ˆå€¼ {CLEANUP_THRESHOLD:.0f}%")
        print(f"   âš ï¸  å¦‚æœä¸æ¸…ç†ï¼Œå¯¹è¯å¯èƒ½éšæ—¶ä¸­æ–­!")

        # ç´§æ€¥æ¸…ç†è®¡åˆ’ï¼ˆç›®æ ‡ä¿ç•™50%å®¹é‡ï¼‰
        TARGET_PERCENT = 50
        target_tokens = int(MAX_TOKENS * (TARGET_PERCENT / 100))

        print(f"\nğŸ“‹ åˆ›å»ºç´§æ€¥æ¸…ç†è®¡åˆ’...")
        print(f"   ç›®æ ‡: å°†ä½¿ç”¨ç‡é™ä½åˆ° {TARGET_PERCENT}%")
        print(f"   ç›®æ ‡token: {target_tokens:,}")

        plan = manager.create_cleanup_plan(messages, target_tokens)

        print(f"\nğŸ”§ æ¸…ç†ç­–ç•¥:")
        print(f"   1. ä¼˜å…ˆæ¸…ç†ä½é‡è¦æ€§æ¶ˆæ¯")
        print(f"   2. æ¸…ç†æ—©æœŸéå…³é”®è®¨è®º")
        print(f"   3. ä¿ç•™æ‰€æœ‰ä»£ç å®ç°")
        print(f"   4. ä¿ç•™æœ€è¿‘çš„é‡è¦å†³ç­–")

        print(f"\nğŸ“Š æ¸…ç†è®¡åˆ’è¯¦æƒ…:")
        print(f"   åŸå§‹æ¶ˆæ¯: {len(messages)}æ¡")
        print(f"   ä¿ç•™æ¶ˆæ¯: {len(plan['messages_to_keep'])}æ¡")
        print(f"   æ¸…ç†æ¶ˆæ¯: {len(plan['messages_to_remove'])}æ¡")
        print(f"   æ¸…ç†æ¯”ä¾‹: {len(plan['messages_to_remove'])/len(messages)*100:.1f}%")
        print(f"   Tokenå‡å°‘: {plan['current_tokens']:,} â†’ {plan['remaining_tokens']:,}")
        print(f"   ä½¿ç”¨ç‡é™ä½: {plan['current_tokens']/MAX_TOKENS*100:.1f}% â†’ {plan['remaining_tokens']/MAX_TOKENS*100:.1f}%")

        # æ‰§è¡Œç´§æ€¥æ¸…ç†
        print(f"\nğŸ”„ æ‰§è¡Œç´§æ€¥æ¸…ç†...")
        cleaned_messages = manager.cleanup_conversation(messages, plan)

        # éªŒè¯ç»“æœ
        cleaned_stats = manager.get_conversation_stats(cleaned_messages)

        print(f"\nâœ… ç´§æ€¥æ¸…ç†å®Œæˆ!")
        print(f"   ğŸ“¦ æ¶ˆæ¯æ•°é‡: {len(messages)} â†’ {len(cleaned_messages)}")
        print(f"   ğŸ§® Tokenæ•°é‡: {plan['current_tokens']:,} â†’ {cleaned_stats['total_tokens']:,}")
        print(f"   ğŸ“ˆ ä½¿ç”¨ç‡: {plan['current_tokens']/MAX_TOKENS*100:.1f}% â†’ {cleaned_stats['token_usage_percent']:.1f}%")

        # æ£€æŸ¥å…³é”®å†…å®¹ä¿ç•™æƒ…å†µ
        code_messages_before = sum(1 for msg in messages if "```" in msg.get("content", ""))
        code_messages_after = sum(1 for msg in cleaned_messages if "```" in msg.get("content", ""))

        print(f"\nğŸ” å…³é”®å†…å®¹ä¿ç•™æ£€æŸ¥:")
        print(f"   ä»£ç å—ä¿ç•™: {code_messages_after}/{code_messages_before} ({code_messages_after/code_messages_before*100:.1f}%)")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ¸…ç†æ€»ç»“
        has_summary = any(msg.get("role") == "system" for msg in cleaned_messages)
        print(f"   æ¸…ç†æ€»ç»“: {'å·²æ·»åŠ ' if has_summary else 'æœªæ·»åŠ '}")

        # æœ€ç»ˆå®‰å…¨çŠ¶æ€
        final_needs_cleanup = manager.should_cleanup(cleaned_messages)

        print(f"\nğŸ‰ æœ€ç»ˆå®‰å…¨çŠ¶æ€:")
        if not final_needs_cleanup:
            print(f"   âœ… ç´§æ€¥çŠ¶æ€è§£é™¤!")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨ç‡: {cleaned_stats['token_usage_percent']:.1f}%")
            print(f"   ğŸ”’ å®‰å…¨è¾¹ç•Œ: {CLEANUP_THRESHOLD - cleaned_stats['token_usage_percent']:.1f}%")
            print(f"   ğŸ¯ å¯ä»¥å®‰å…¨ç»§ç»­å¯¹è¯")
        else:
            print(f"   âš ï¸  ä»éœ€è¿›ä¸€æ­¥æ¸…ç†")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨ç‡: {cleaned_stats['token_usage_percent']:.1f}%")
            print(f"   ğŸš¨ ä»è¶…è¿‡é˜ˆå€¼: {cleaned_stats['token_usage_percent'] - CLEANUP_THRESHOLD:.1f}%")

        # æ˜¾ç¤ºæ¸…ç†åçš„æ¶ˆæ¯æ‘˜è¦
        print(f"\nğŸ“ æ¸…ç†åå¯¹è¯æ‘˜è¦:")
        for i, msg in enumerate(cleaned_messages[:5]):  # åªæ˜¾ç¤ºå‰5æ¡
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            preview = content[:80] + "..." if len(content) > 80 else content

            icon = "ğŸ’" if "```" in content else "ğŸ“"
            print(f"   {icon} [{i:2d}] {role}: {preview}")

        if len(cleaned_messages) > 5:
            print(f"   ... è¿˜æœ‰ {len(cleaned_messages)-5} æ¡æ¶ˆæ¯")

    else:
        print(f"\nâœ… å½“å‰çŠ¶æ€æ­£å¸¸ï¼Œæ— éœ€ç´§æ€¥æ¸…ç†")
        print(f"   ä½¿ç”¨ç‡: {stats['token_usage_percent']:.1f}%")
        print(f"   å®‰å…¨è¾¹ç•Œ: {CLEANUP_THRESHOLD - stats['token_usage_percent']:.1f}%")

    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"   1. å®šæœŸä½¿ç”¨ `/conversation-stats` æ£€æŸ¥çŠ¶æ€")
    print(f"   2. å½“ä½¿ç”¨ç‡è¶…è¿‡70%æ—¶ä¸»åŠ¨æ¸…ç†")
    print(f"   3. é‡è¦å†…å®¹ä½¿ç”¨ç‰¹å®šæ ¼å¼æ ‡è®°")
    print(f"   4. é•¿æ—¶é—´å¯¹è¯åˆ†æ®µè¿›è¡Œ")

    print(f"\n" + "=" * 90)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 90)

if __name__ == "__main__":
    test_critical_scenario()